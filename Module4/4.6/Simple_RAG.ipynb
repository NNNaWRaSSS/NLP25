{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "!pip install faiss-cpu # Install FAISS library"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4OO_8xNWPc4",
        "outputId": "dd887d95-d9f7-4da6-8e5e-72e79ee8535a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyv6c4LoVtvU",
        "outputId": "0d623674-0c00-436a-abdf-008637594807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Where is the Eiffel Tower located?\n",
            "Response: The Mona Lisa is displayed in the Louvre Museum. The Eiffel Tower is located in Paris, France. Where is the Eiffer Tower located? Visit CNN.com/EifferTower for more information.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create a small knowledge base (document collection)\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Great Wall of China is over 13,000 miles long.\",\n",
        "    \"Python is a popular programming language for machine learning.\",\n",
        "    \"The Eiffel Tower is located in Paris, France.\",\n",
        "    \"The Mona Lisa is displayed in the Louvre Museum.\"\n",
        "]\n",
        "\n",
        "# Step 2: Set up FAISS (Retriever)\n",
        "# Convert documents to embeddings (for simplicity, use numeric indices as embeddings here)\n",
        "document_embeddings = np.array([i for i in range(len(knowledge_base))], dtype=\"float32\").reshape(-1, 1)\n",
        "faiss_index = faiss.IndexFlatL2(1)\n",
        "faiss_index.add(document_embeddings)\n",
        "\n",
        "# Step 3: Initialize the generator (Hugging Face model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Step 4: Define a RAG pipeline\n",
        "def rag_pipeline(query):\n",
        "    # Fake query embedding (for simplicity, map the query length to an embedding)\n",
        "    query_embedding = np.array([[len(query.split())]], dtype=\"float32\")\n",
        "\n",
        "    # Retrieve top-k documents\n",
        "    top_k = 2\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "    retrieved_docs = [knowledge_base[idx] for idx in indices[0]]\n",
        "\n",
        "    # Combine retrieved documents as context\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    # Generate response using the context and query\n",
        "    input_text = f\"Context: {context} Question: {query}\"\n",
        "    response = generator(input_text, max_length=50, truncation=True)[0]['generated_text']\n",
        "\n",
        "    return response\n",
        "\n",
        "# Step 5: Test the RAG system\n",
        "query = \"Where is the Eiffel Tower located?\"\n",
        "response = rag_pipeline(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "source": [
        "!pip install -U sentence-transformers faiss-cpu"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "iUp_s0CqX_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Step 1: Create a small knowledge base (document collection)\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Great Wall of China is over 13,000 miles long.\",\n",
        "    \"Python is a popular programming language for machine learning.\",\n",
        "    \"The Eiffel Tower is located in Paris, France.\",\n",
        "    \"The Mona Lisa is displayed in the Louvre Museum.\"\n",
        "]\n",
        "\n",
        "# Step 2: Set up FAISS (Retriever) with Sentence-BERT embeddings\n",
        "# Initialize Sentence-BERT model\n",
        "model_name = 'all-mpnet-base-v2'  # Choose a suitable Sentence-BERT model\n",
        "encoder = SentenceTransformer(model_name)\n",
        "\n",
        "# Generate document embeddings\n",
        "document_embeddings = encoder.encode(knowledge_base, convert_to_tensor=True)\n",
        "document_embeddings = document_embeddings.cpu().numpy() # Convert to numpy array\n",
        "\n",
        "# Create FAISS index\n",
        "faiss_index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
        "faiss_index.add(document_embeddings)\n",
        "\n",
        "# Step 3: Initialize the generator (Hugging Face model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Step 4: Define a RAG pipeline\n",
        "def rag_pipeline(query):\n",
        "    # Generate query embedding using Sentence-BERT\n",
        "    query_embedding = encoder.encode(query, convert_to_tensor=True)\n",
        "    query_embedding = query_embedding.cpu().numpy() # Convert to numpy array\n",
        "\n",
        "    # Retrieve top-k documents\n",
        "    top_k = 2\n",
        "    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), top_k) # Reshape query embedding\n",
        "    retrieved_docs = [knowledge_base[idx] for idx in indices[0]]\n",
        "\n",
        "    # Combine retrieved documents as context\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    # Generate response using the context and query\n",
        "    input_text = f\"Context: {context} Question: {query}\"\n",
        "    response = generator(input_text, max_length=50, truncation=True)[0]['generated_text']\n",
        "\n",
        "    return response\n",
        "\n",
        "# Step 5: Test the RAG system\n",
        "query = \"Where is the Eiffel Tower located?\"\n",
        "response = rag_pipeline(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3Ds8JGIYsWb",
        "outputId": "f2ac2327-6307-40ad-80ac-7317a6ae45f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1484: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Where is the Eiffel Tower located?\n",
            "Response: The Eiffel Tower is located in Paris, France. The capital of France is Paris. It is located on the banks of the River Seine, in the center of the city. It was built in 1903 and is one of\n"
          ]
        }
      ]
    }
  ]
}